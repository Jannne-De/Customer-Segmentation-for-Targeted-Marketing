{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\n\n\n# Load dataset\nfile_path = \"/kaggle/input/online-retail-dataset/online_retail.csv\"\ndf = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n\n# Display basic information\nprint(df.head())\nprint(df.info())\nprint(\"Missing values:\\n\", df.isnull().sum())\nprint(\"Unique product categories:\", df[\"Description\"].nunique())\nprint(\"Number of unique customers:\", df[\"CustomerID\"].nunique())\nprint(\"Date range:\", df[\"InvoiceDate\"].min(), \"-\", df[\"InvoiceDate\"].max())\n\n# Data Cleaning\nprint(\"Cleaning data...\")\ndf_cleaned = df.copy().dropna(subset=[\"Description\", \"CustomerID\"])\ndf_cleaned[\"InvoiceDate\"] = pd.to_datetime(df_cleaned[\"InvoiceDate\"])\ndf_cleaned = df_cleaned[(df_cleaned[\"Quantity\"] > 0) & (df_cleaned[\"UnitPrice\"] > 0)]\n\n# Visualization: Monthly Sales\nplt.figure(figsize=(12, 6))\ndf_cleaned.set_index(\"InvoiceDate\")[\"Quantity\"].resample(\"M\").sum().plot()\nplt.title(\"Monthly Sales Volume\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Quantity Sold\")\nplt.show()\n\n# Visualization: Product Price Distribution\nplt.figure(figsize=(10, 5))\nsns.histplot(df_cleaned[\"UnitPrice\"], bins=100, kde=True)\nplt.xlim(0, 100)\nplt.title(\"Product Price Distribution\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Top 10 Products by Sales\ntop_products = df_cleaned.groupby(\"Description\")[\"Quantity\"].sum().sort_values(ascending=False).head(10)\nplt.figure(figsize=(10, 5))\nsns.barplot(y=top_products.index, x=top_products.values, palette=\"Blues_r\")\nplt.xlabel(\"Quantity Sold\")\nplt.ylabel(\"Product\")\nplt.title(\"Top 10 Best-Selling Products\")\nplt.show()\n\n# Customer Spending Analysis\ndf_cleaned[\"TotalPrice\"] = df_cleaned[\"Quantity\"] * df_cleaned[\"UnitPrice\"]\ncustomer_spending = df_cleaned.groupby(\"CustomerID\")[\"TotalPrice\"].sum()\n\nplt.figure(figsize=(10, 5))\nsns.histplot(customer_spending, bins=100, kde=True)\nplt.xlim(0, 2000)\nplt.title(\"Customer Spending Distribution\")\nplt.xlabel(\"Total Spending\")\nplt.ylabel(\"Number of Customers\")\nplt.show()\n\n# Outlier Handling\nprice_limit = df_cleaned[\"UnitPrice\"].quantile(0.99)\nplt.figure(figsize=(10, 5))\nsns.histplot(df_cleaned[df_cleaned[\"UnitPrice\"] < price_limit][\"UnitPrice\"], bins=50, kde=True)\nplt.title(\"Price Distribution (Without Outliers)\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# RFM Analysis\ncurrent_date = df_cleaned[\"InvoiceDate\"].max()\nrfm = df_cleaned.groupby(\"CustomerID\").agg({\n    \"InvoiceDate\": lambda x: (current_date - x.max()).days,\n    \"InvoiceNo\": \"count\",\n    \"TotalPrice\": \"sum\"\n}).reset_index()\nrfm.columns = [\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\"]\nprint(rfm.describe())\n\n# Scaling for Clustering\nscaler = StandardScaler()\nrfm_scaled = scaler.fit_transform(rfm[[\"Recency\", \"Frequency\", \"Monetary\"]])\n\n# Elbow Method to Find Optimal k\nwcss = []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(rfm_scaled)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 11), wcss, marker=\"o\", linestyle=\"--\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"WCSS (Inertia)\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.show()\n\n# Clustering\noptimal_k = 4\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\nrfm[\"Cluster\"] = kmeans.fit_predict(rfm_scaled)\nprint(rfm[\"Cluster\"].value_counts())\n\n# Visualizing Clusters\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=rfm[\"Recency\"], y=rfm[\"Monetary\"], hue=rfm[\"Cluster\"], palette=\"Set2\", alpha=0.7)\nplt.title(\"Customer Segmentation (Recency vs Monetary)\")\nplt.xlabel(\"Recency (days since last purchase)\")\nplt.ylabel(\"Monetary (Total Spending)\")\nplt.legend(title=\"Cluster\")\nplt.show()\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=rfm[\"Frequency\"], y=rfm[\"Monetary\"], hue=rfm[\"Cluster\"], palette=\"Set1\", alpha=0.7)\nplt.title(\"Customer Segmentation (Frequency vs Monetary)\")\nplt.xlabel(\"Frequency (Number of Purchases)\")\nplt.ylabel(\"Monetary (Total Spending)\")\nplt.legend(title=\"Cluster\")\nplt.show()\n\n# Cluster Summary\nrfm_cluster_summary = rfm.groupby(\"Cluster\")[[\"Recency\", \"Frequency\", \"Monetary\"]].mean()\nplt.figure(figsize=(12, 5))\nsns.heatmap(rfm_cluster_summary, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\nplt.title(\"Average RFM Values by Cluster\")\nplt.show()\n\n\n# Check available columns in rfm\necho \"List of columns in rfm:\", rfm.columns.tolist()\n\n# Rename 'TotalOrders_y' if it exists\nif \"TotalOrders_y\" in rfm.columns:\n    rfm.rename(columns={\"TotalOrders_y\": \"TotalOrders\"}, inplace=True)\nelif \"TotalOrders_x\" in rfm.columns:\n    rfm.rename(columns={\"TotalOrders_x\": \"TotalOrders\"}, inplace=True)\n\n# Create 'TotalOrders' column if missing\nif \"TotalOrders\" not in rfm.columns:\n    print(\"⚠️ Warning! 'TotalOrders' column is missing. Creating it with default value 0.\")\n    rfm[\"TotalOrders\"] = 0\n\n# Special Customer Analysis\nspecial_customers = [12748, 14911]\nspecial_customers_df = df_cleaned[df_cleaned[\"CustomerID\"].isin(special_customers)]\n\n# Calculate key metrics\nspecial_customers_analysis = special_customers_df.groupby(\"CustomerID\").agg(\n    Total_Spending=(\"TotalPrice\", \"sum\"),\n    Total_Orders=(\"InvoiceNo\", \"nunique\"),\n    Unique_Products=(\"Description\", \"nunique\"),\n    Total_Quantity=(\"Quantity\", \"sum\")\n)\n\n# Save analysis to a CSV file\nspecial_customers_analysis.to_csv(\"special_customers_analysis.csv\")\n\n# Identify Enterprise Customers\nenterprise_customers = rfm[(rfm[\"TotalOrders\"] > 200) & (rfm[\"Monetary\"] > 30000)][\"CustomerID\"]\nprint(\"Enterprise Customers:\", enterprise_customers.tolist())\n\n# Assign Enterprise label to identified customers\nrfm[\"Cluster\"] = rfm.apply(lambda row: \"Enterprise\" if row[\"CustomerID\"] in enterprise_customers.tolist() else row[\"Cluster\"], axis=1)\n\n# Identify Wholesale Customers\nwholesale_customers = [14646, 18102, 17450, 16446, 14911, 12415, 14156, 17511, 16029, 15098]\nrfm[\"Cluster\"] = rfm.apply(lambda row: \"Wholesale\" if row[\"CustomerID\"] in wholesale_customers else row[\"Cluster\"], axis=1)\n\n# Merge small clusters into one called \"Special\"\nfinal_df = rfm.drop(columns=[\"CustomerID\"], errors=\"ignore\")\nfinal_df[\"Cluster\"] = final_df[\"Cluster\"].replace({\"Enterprise\": \"Special\", \"Wholesale\": \"Special\", 3: 2})\n\n# Convert 'Special' to numerical representation\nfinal_df[\"Cluster\"] = final_df[\"Cluster\"].replace(\"Special\", 2).infer_objects(copy=False).astype(int)\nprint(\"Cluster distribution after merging minor classes:\")\nprint(final_df[\"Cluster\"].value_counts())\n\n# Encode Clusters\nle = LabelEncoder()\nfinal_df[\"Cluster\"] = le.fit_transform(final_df[\"Cluster\"])\nprint(\"Unique cluster values after encoding:\", final_df[\"Cluster\"].unique())\n\n# Split data into train and test sets\nX = final_df.drop(columns=[\"Cluster\"], errors=\"ignore\")\ny = final_df[\"Cluster\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Display dataset shapes\nprint(\"Shapes of X_train, X_test, y_train, y_test:\", X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n# Class distribution\nprint(\"Class distribution in y_train:\")\nprint(pd.Series(y_train).value_counts())\nprint(\"Class distribution in y_test:\")\nprint(pd.Series(y_test).value_counts())\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}